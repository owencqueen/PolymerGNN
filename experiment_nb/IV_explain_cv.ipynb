{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from polymerlearn.utils import get_IV_add, GraphDataset\n",
    "\n",
    "# Load data from local path:\n",
    "data = pd.read_csv(os.path.join('/Users/owenqueen/Desktop/eastman_project-confidential/Eastman_Project/CombinedData', \n",
    "            'pub_data.csv'))\n",
    "\n",
    "add = get_IV_add(data)\n",
    "\n",
    "dataset = GraphDataset(\n",
    "    data = data,\n",
    "    structure_dir = '../Structures/AG/xyz',\n",
    "    Y_target=['IV'],\n",
    "    test_size = 0.2,\n",
    "    add_features=add\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenqueen/Desktop/Eastman_project-Confidential/Eastman_Project/PolymerGNN/polymerlearn/utils/graph_prep.py:425: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1640811925055/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  return self.test_data, torch.tensor(self.Ytest).float(), self.add_test, test_inds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 \t Epoch: 0,                     \t Train r2: -22.1721 \t Train Loss: 44.6549\n",
      "Fold: 1 \t Epoch: 50,                     \t Train r2: 0.3647 \t Train Loss: 2.7650\n",
      "Fold: 1 \t Epoch: 100,                     \t Train r2: 0.4047 \t Train Loss: 1.2710\n",
      "Fold: 1 \t Epoch: 150,                     \t Train r2: 0.6124 \t Train Loss: 1.1223\n",
      "Fold: 1 \t Epoch: 200,                     \t Train r2: 0.6868 \t Train Loss: 1.4641\n",
      "Fold: 1 \t Epoch: 250,                     \t Train r2: 0.6115 \t Train Loss: 1.4694\n",
      "Fold: 1 \t Epoch: 300,                     \t Train r2: 0.6351 \t Train Loss: 0.9036\n",
      "Fold: 1 \t Epoch: 350,                     \t Train r2: 0.8235 \t Train Loss: 0.4980\n",
      "Fold: 1 \t Epoch: 400,                     \t Train r2: 0.7001 \t Train Loss: 1.2849\n",
      "Fold: 1 \t Epoch: 450,                     \t Train r2: 0.7411 \t Train Loss: 0.7745\n",
      "Fold: 1 \t Epoch: 500,                     \t Train r2: 0.7871 \t Train Loss: 0.5950\n",
      "Fold: 1 \t Epoch: 550,                     \t Train r2: 0.7716 \t Train Loss: 0.5620\n",
      "Fold: 1 \t Epoch: 600,                     \t Train r2: 0.6797 \t Train Loss: 0.7150\n",
      "Fold: 1 \t Epoch: 650,                     \t Train r2: 0.7762 \t Train Loss: 0.6330\n",
      "Fold: 1 \t Epoch: 700,                     \t Train r2: 0.8685 \t Train Loss: 0.4091\n",
      "Fold: 1 \t Epoch: 750,                     \t Train r2: 0.7958 \t Train Loss: 0.5390\n",
      "Fold: 1 \t Epoch: 800,                     \t Train r2: 0.7811 \t Train Loss: 0.5715\n",
      "Fold: 1 \t Epoch: 850,                     \t Train r2: 0.9042 \t Train Loss: 0.2855\n",
      "Fold: 1 \t Epoch: 900,                     \t Train r2: 0.7584 \t Train Loss: 0.6925\n",
      "Fold: 1 \t Epoch: 950,                     \t Train r2: 0.8400 \t Train Loss: 0.6660\n",
      "Fold: 1 \t Test r2: 0.8537 \t Test Loss: 0.0059 \t Test MAE: 0.0560\n",
      "Fold: 2 \t Epoch: 0,                     \t Train r2: -2.0200 \t Train Loss: 8.6488\n",
      "Fold: 2 \t Epoch: 50,                     \t Train r2: 0.5301 \t Train Loss: 1.7561\n",
      "Fold: 2 \t Epoch: 100,                     \t Train r2: 0.6940 \t Train Loss: 0.8511\n",
      "Fold: 2 \t Epoch: 150,                     \t Train r2: 0.6774 \t Train Loss: 0.7574\n",
      "Fold: 2 \t Epoch: 200,                     \t Train r2: 0.6274 \t Train Loss: 1.4513\n",
      "Fold: 2 \t Epoch: 250,                     \t Train r2: 0.7692 \t Train Loss: 0.5071\n",
      "Fold: 2 \t Epoch: 300,                     \t Train r2: 0.7724 \t Train Loss: 0.8577\n",
      "Fold: 2 \t Epoch: 350,                     \t Train r2: 0.7609 \t Train Loss: 0.5813\n",
      "Fold: 2 \t Epoch: 400,                     \t Train r2: 0.8295 \t Train Loss: 0.4282\n",
      "Fold: 2 \t Epoch: 450,                     \t Train r2: 0.7995 \t Train Loss: 0.4939\n",
      "Fold: 2 \t Epoch: 500,                     \t Train r2: 0.8204 \t Train Loss: 0.6750\n",
      "Fold: 2 \t Epoch: 550,                     \t Train r2: 0.8514 \t Train Loss: 0.6026\n",
      "Fold: 2 \t Epoch: 600,                     \t Train r2: 0.8168 \t Train Loss: 0.4976\n",
      "Fold: 2 \t Epoch: 650,                     \t Train r2: 0.8368 \t Train Loss: 0.4319\n",
      "Fold: 2 \t Epoch: 700,                     \t Train r2: 0.8674 \t Train Loss: 0.3555\n",
      "Fold: 2 \t Epoch: 750,                     \t Train r2: 0.8187 \t Train Loss: 0.4397\n",
      "Fold: 2 \t Epoch: 800,                     \t Train r2: 0.8474 \t Train Loss: 0.4369\n",
      "Fold: 2 \t Epoch: 850,                     \t Train r2: 0.8631 \t Train Loss: 0.3811\n",
      "Fold: 2 \t Epoch: 900,                     \t Train r2: 0.8849 \t Train Loss: 0.3582\n",
      "Fold: 2 \t Epoch: 950,                     \t Train r2: 0.8574 \t Train Loss: 0.3894\n",
      "Fold: 2 \t Test r2: 0.8597 \t Test Loss: 0.0066 \t Test MAE: 0.0499\n",
      "Fold: 3 \t Epoch: 0,                     \t Train r2: -35.9254 \t Train Loss: 76.9528\n",
      "Fold: 3 \t Epoch: 50,                     \t Train r2: 0.2837 \t Train Loss: 1.2367\n",
      "Fold: 3 \t Epoch: 100,                     \t Train r2: 0.5406 \t Train Loss: 1.5011\n",
      "Fold: 3 \t Epoch: 150,                     \t Train r2: 0.7071 \t Train Loss: 1.0736\n",
      "Fold: 3 \t Epoch: 200,                     \t Train r2: 0.6995 \t Train Loss: 0.7205\n",
      "Fold: 3 \t Epoch: 250,                     \t Train r2: 0.7425 \t Train Loss: 0.7800\n",
      "Fold: 3 \t Epoch: 300,                     \t Train r2: 0.7717 \t Train Loss: 0.6753\n",
      "Fold: 3 \t Epoch: 350,                     \t Train r2: 0.6657 \t Train Loss: 0.7793\n",
      "Fold: 3 \t Epoch: 400,                     \t Train r2: 0.8303 \t Train Loss: 0.6444\n",
      "Fold: 3 \t Epoch: 450,                     \t Train r2: 0.8002 \t Train Loss: 0.5089\n",
      "Fold: 3 \t Epoch: 500,                     \t Train r2: 0.8559 \t Train Loss: 0.5536\n",
      "Fold: 3 \t Epoch: 550,                     \t Train r2: 0.8449 \t Train Loss: 0.4513\n",
      "Fold: 3 \t Epoch: 600,                     \t Train r2: 0.8523 \t Train Loss: 0.3507\n",
      "Fold: 3 \t Epoch: 650,                     \t Train r2: 0.8539 \t Train Loss: 0.2999\n",
      "Fold: 3 \t Epoch: 700,                     \t Train r2: 0.8481 \t Train Loss: 0.3178\n",
      "Fold: 3 \t Epoch: 750,                     \t Train r2: 0.8382 \t Train Loss: 0.4358\n",
      "Fold: 3 \t Epoch: 800,                     \t Train r2: 0.8518 \t Train Loss: 0.4058\n",
      "Fold: 3 \t Epoch: 850,                     \t Train r2: 0.8621 \t Train Loss: 0.5424\n",
      "Fold: 3 \t Epoch: 900,                     \t Train r2: 0.9008 \t Train Loss: 0.3090\n",
      "Fold: 3 \t Epoch: 950,                     \t Train r2: 0.8665 \t Train Loss: 0.5816\n",
      "Fold: 3 \t Test r2: 0.7187 \t Test Loss: 0.0126 \t Test MAE: 0.0638\n",
      "Fold: 4 \t Epoch: 0,                     \t Train r2: -6.9013 \t Train Loss: 18.2445\n",
      "Fold: 4 \t Epoch: 50,                     \t Train r2: 0.3923 \t Train Loss: 2.4272\n",
      "Fold: 4 \t Epoch: 100,                     \t Train r2: 0.6056 \t Train Loss: 0.8484\n",
      "Fold: 4 \t Epoch: 150,                     \t Train r2: 0.6277 \t Train Loss: 1.5263\n",
      "Fold: 4 \t Epoch: 200,                     \t Train r2: 0.5597 \t Train Loss: 0.9951\n",
      "Fold: 4 \t Epoch: 250,                     \t Train r2: 0.7264 \t Train Loss: 0.6530\n",
      "Fold: 4 \t Epoch: 300,                     \t Train r2: 0.7350 \t Train Loss: 0.9567\n",
      "Fold: 4 \t Epoch: 350,                     \t Train r2: 0.7146 \t Train Loss: 1.2378\n",
      "Fold: 4 \t Epoch: 400,                     \t Train r2: 0.7140 \t Train Loss: 1.0740\n",
      "Fold: 4 \t Epoch: 450,                     \t Train r2: 0.7538 \t Train Loss: 0.8240\n",
      "Fold: 4 \t Epoch: 500,                     \t Train r2: 0.8108 \t Train Loss: 0.8207\n",
      "Fold: 4 \t Epoch: 550,                     \t Train r2: 0.8046 \t Train Loss: 0.4558\n",
      "Fold: 4 \t Epoch: 600,                     \t Train r2: 0.8093 \t Train Loss: 0.4771\n",
      "Fold: 4 \t Epoch: 650,                     \t Train r2: 0.8097 \t Train Loss: 0.4509\n",
      "Fold: 4 \t Epoch: 700,                     \t Train r2: 0.8133 \t Train Loss: 0.4366\n",
      "Fold: 4 \t Epoch: 750,                     \t Train r2: 0.7351 \t Train Loss: 0.4785\n",
      "Fold: 4 \t Epoch: 800,                     \t Train r2: 0.8449 \t Train Loss: 0.5933\n",
      "Fold: 4 \t Epoch: 850,                     \t Train r2: 0.9075 \t Train Loss: 0.3100\n",
      "Fold: 4 \t Epoch: 900,                     \t Train r2: 0.8637 \t Train Loss: 0.3043\n",
      "Fold: 4 \t Epoch: 950,                     \t Train r2: 0.9277 \t Train Loss: 0.1918\n",
      "Fold: 4 \t Test r2: 0.6345 \t Test Loss: 0.0140 \t Test MAE: 0.0753\n",
      "Fold: 5 \t Epoch: 0,                     \t Train r2: -2.9581 \t Train Loss: 13.3792\n",
      "Fold: 5 \t Epoch: 50,                     \t Train r2: 0.5254 \t Train Loss: 1.0464\n",
      "Fold: 5 \t Epoch: 100,                     \t Train r2: 0.5296 \t Train Loss: 1.0275\n",
      "Fold: 5 \t Epoch: 150,                     \t Train r2: 0.6256 \t Train Loss: 1.1516\n",
      "Fold: 5 \t Epoch: 200,                     \t Train r2: 0.6738 \t Train Loss: 0.8375\n",
      "Fold: 5 \t Epoch: 250,                     \t Train r2: 0.7071 \t Train Loss: 0.8305\n",
      "Fold: 5 \t Epoch: 300,                     \t Train r2: 0.7649 \t Train Loss: 0.7144\n",
      "Fold: 5 \t Epoch: 350,                     \t Train r2: 0.8504 \t Train Loss: 0.4158\n",
      "Fold: 5 \t Epoch: 400,                     \t Train r2: 0.8289 \t Train Loss: 0.4920\n",
      "Fold: 5 \t Epoch: 450,                     \t Train r2: 0.9088 \t Train Loss: 0.2452\n",
      "Fold: 5 \t Epoch: 500,                     \t Train r2: 0.9189 \t Train Loss: 0.2544\n",
      "Fold: 5 \t Epoch: 550,                     \t Train r2: 0.8643 \t Train Loss: 0.4113\n",
      "Fold: 5 \t Epoch: 600,                     \t Train r2: 0.9062 \t Train Loss: 0.2056\n",
      "Fold: 5 \t Epoch: 650,                     \t Train r2: 0.9130 \t Train Loss: 0.2042\n",
      "Fold: 5 \t Epoch: 700,                     \t Train r2: 0.9143 \t Train Loss: 0.2461\n",
      "Fold: 5 \t Epoch: 750,                     \t Train r2: 0.9145 \t Train Loss: 0.2143\n",
      "Fold: 5 \t Epoch: 800,                     \t Train r2: 0.9253 \t Train Loss: 0.1739\n",
      "Fold: 5 \t Epoch: 850,                     \t Train r2: 0.9467 \t Train Loss: 0.1415\n",
      "Fold: 5 \t Epoch: 900,                     \t Train r2: 0.9205 \t Train Loss: 0.2021\n",
      "Fold: 5 \t Epoch: 950,                     \t Train r2: 0.9265 \t Train Loss: 0.2294\n",
      "Fold: 5 \t Test r2: 0.6096 \t Test Loss: 0.0250 \t Test MAE: 0.0742\n",
      "Final avg. r2:  0.7352555355961996\n",
      "Final avg. MSE: 0.012829714005289466\n",
      "Final avg. MAE: 0.06384283126059438\n"
     ]
    }
   ],
   "source": [
    "from polymerlearn.models.gnn import PolymerGNN_IV\n",
    "from polymerlearn.utils import CV_eval\n",
    "\n",
    "model_kwargs = {\n",
    "    'input_feat': 6,         # How many input features on each node; don't change this\n",
    "    'hidden_channels': 32,   # How many intermediate dimensions to use in model\n",
    "                            # Can change this ^^\n",
    "    'num_additional': 4      # How many additional resin properties to include in the prediction\n",
    "                            # Corresponds to the number in get_IV_add\n",
    "}\n",
    "\n",
    "model = PolymerGNN_IV(**model_kwargs)\n",
    "\n",
    "optimizer_gen = torch.optim.AdamW\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "all_predictions, all_y, all_inds, state_dicts = CV_eval(\n",
    "    dataset = dataset,\n",
    "    model_generator = PolymerGNN_IV,\n",
    "    optimizer_generator = optimizer_gen,\n",
    "    criterion = criterion,\n",
    "    model_generator_kwargs = model_kwargs,\n",
    "    optimizer_kwargs = {'lr': 0.0001, 'weight_decay':0.01},\n",
    "    epochs = 800,\n",
    "    batch_size = 64,\n",
    "    verbose = 1,\n",
    "    save_state_dicts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(state_dicts, open('state_dicts.pt', 'wb')) # Save state dicts to load later\n",
    "torch.save(dataset, 'dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenqueen/Desktop/Eastman_project-Confidential/Eastman_Project/PolymerGNN/polymerlearn/explain/explain_gnn.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(add_test).float())\n"
     ]
    }
   ],
   "source": [
    "from polymerlearn.explain import PolymerGNN_IV_EXPLAIN, PolymerGNNExplainer\n",
    "\n",
    "mexplain = PolymerGNN_IV_EXPLAIN(**model_kwargs)\n",
    "mexplain.load_state_dict(model.state_dict()) # Load weights from trained model over to explaining one\n",
    "\n",
    "explainer = PolymerGNNExplainer(mexplain)\n",
    "\n",
    "test_batch, Ytest, add_test = dataset.get_test()\n",
    "test_inds = dataset.test_mask\n",
    "\n",
    "exp_summary = []\n",
    "\n",
    "for i in range(Ytest.shape[0]):\n",
    "    scores = explainer.get_explanation(test_batch[i], add_test[i])\n",
    "    scores['A'] = torch.sum(scores['A'], dim = 1)\n",
    "    scores['G'] = torch.sum(scores['G'], dim = 1)\n",
    "    scores['table_ind'] = test_inds[i]\n",
    "\n",
    "    exp_summary.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5220816731452942, -0.871830940246582, -1.372873067855835, -0.7051889300346375, -1.479537844657898, -0.8235787749290466, -0.49047982692718506, -0.4644251763820648, -1.7646515369415283, -1.5553362369537354, -1.144914150238037, -2.20985746383667, -1.4301812648773193, -0.5704778432846069, -0.9277279376983643, -0.6290164589881897, -0.811713457107544, -0.4857237935066223, -1.2340717315673828, -0.7832549214363098, -2.0962419509887695, -0.3163483738899231, -0.6890739798545837, -1.1152818202972412, -0.6931972503662109, -0.519429087638855, -0.70504230260849, -0.5535339117050171, -0.7099688053131104, -0.5150319337844849, -1.429985761642456, -0.777635931968689, -0.45038893818855286, -0.8560777902603149, -0.5053524971008301, -0.9263220429420471, -0.41691508889198303, -1.2905853986740112, -1.724608302116394, -1.264465093612671, -1.064122200012207, -0.8285700678825378, -0.9279764890670776, -0.7006344199180603, -2.206166982650757, -0.6776403188705444, -2.2430825233459473, -0.9685702323913574, -2.1730713844299316]\n",
      "[-0.19358092546463013, -0.31538155674934387, -0.4075339436531067, -0.24636363983154297, -0.16116763651371002, -0.2500002384185791, -0.1836758404970169, -0.1832408905029297, -0.289745569229126, -0.4898459315299988, -0.3457842469215393, -0.09748543053865433, -0.24077792465686798, -0.1872561275959015, -0.12349295616149902, -0.22302083671092987, -0.26418301463127136, -0.1691029965877533, -0.4575773775577545, -0.2904205620288849, -0.35939761996269226, -0.09112056344747543, -0.25549939274787903, -0.40344905853271484, -0.25702834129333496, -0.1925973892211914, -0.26142024993896484, -0.21236635744571686, -0.26587051153182983, -0.19096697866916656, -0.24133965373039246, -0.2883370518684387, -0.16866253316402435, -0.3174222707748413, -0.18924538791179657, -0.3434678912162781, -0.10272779315710068, -0.37255603075027466, -0.2836940884590149, -0.4688469171524048, -0.42410820722579956, -0.2660190165042877, -0.32556864619255066, -0.26237499713897705, -0.2911989986896515, -0.2132880538702011, -0.2904195785522461, -0.35037675499916077, -0.18805375695228577]\n",
      "[0.7059991955757141, 1.1890240907669067, 2.005154609680176, 0.9321068525314331, 2.0319747924804688, 1.0864759683609009, 0.6632646918296814, 0.5958696007728577, 2.648825168609619, 2.4161291122436523, 1.6239137649536133, 3.260350465774536, 2.0193707942962646, 0.6346941590309143, 1.166871428489685, 0.8049064874649048, 1.0142182111740112, 0.6417993307113647, 1.668807029724121, 1.0591778755187988, 3.3014161586761475, 0.30126750469207764, 0.931818962097168, 1.5937443971633911, 0.9373953342437744, 0.7024122476577759, 0.9534127116203308, 0.7260822057723999, 0.9600745439529419, 0.6964661478996277, 1.9998971223831177, 1.051579475402832, 0.6090506911277771, 1.1576547622680664, 0.683376669883728, 1.2526445388793945, 0.4081835150718689, 1.9697016477584839, 2.5244998931884766, 1.7099077701568604, 1.4329311847686768, 1.0910775661468506, 1.2548816204071045, 0.9474520683288574, 2.2576370239257812, 0.8280795812606812, 3.4275338649749756, 1.365840196609497, 3.201616048812866]\n",
      "[-0.07748078554868698, 0.010476435534656048, -0.1635705828666687, -0.06612090021371841, -0.10808213800191879, -0.1267797350883484, -0.07865428924560547, -0.056069545447826385, -0.1552579402923584, -0.15307149291038513, -0.13032180070877075, -0.20338153839111328, -0.11930190771818161, -0.03776582330465317, -0.08706940710544586, -0.06983032077550888, -0.0023000070359557867, -0.09305182099342346, 0.02722134068608284, -0.06517401337623596, -0.1844320446252823, -0.025231139734387398, -0.07017552107572556, 0.025654099881649017, -0.0649535059928894, -0.09067077189683914, -0.05386967957019806, -0.025099165737628937, -0.07461056858301163, -0.09324397146701813, -0.12071261554956436, -0.11869584769010544, -0.07948336005210876, -0.050791021436452866, -0.08882081508636475, -0.11898224800825119, -0.027310915291309357, -0.12439130246639252, -0.14558321237564087, 0.004838619381189346, -0.11159058660268784, -0.04421091452240944, -0.09570912271738052, -0.07266847789287567, 0.0022639771923422813, -0.04577193036675453, -0.18976622819900513, -0.10648562014102936, -0.183843195438385]\n",
      "[-0.23808974027633667, -0.42200836539268494, -0.4902564287185669, -0.2829187214374542, -0.6030522584915161, -0.2798883318901062, -0.22754992544651031, -0.20630736649036407, -0.7235944867134094, -0.6330674886703491, -0.44295573234558105, -0.9114898443222046, -0.5769447684288025, -0.15681029856204987, -0.4143129885196686, -0.2408953458070755, -0.312211811542511, -0.20586660504341125, -0.37285614013671875, -0.28846275806427, -0.8595629334449768, -0.12201797962188721, -0.2742249667644501, -0.4859367311000824, -0.27994847297668457, -0.224294051527977, -0.29242148995399475, -0.27281540632247925, -0.26961275935173035, -0.2207055687904358, -0.5625925064086914, -0.23451504111289978, -0.21617382764816284, -0.3070274591445923, -0.22097621858119965, -0.24065172672271729, -0.1367741823196411, -0.5434624552726746, -0.6785044074058533, -0.34738561511039734, -0.21381597220897675, -0.3714749217033386, -0.29613742232322693, -0.2705255448818207, -0.05690170079469681, -0.3021281063556671, -0.8844234347343445, -0.348070353269577, -0.8568188548088074]\n",
      "[0.0, 0.03351076692342758, 0.07162146270275116, 0.008384385146200657, 0.0, 0.021967964246869087, 0.0, 0.0, 0.0, 0.09804382920265198, 0.06583348661661148, 0.0, 0.0, 0.0, 0.0, 0.010201496072113514, 0.010114677250385284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012003138661384583, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.006530595477670431, 0.0, 0.0, 0.0, 0.04795505106449127, 0.013681893236935139, 0.0, 0.0, 0.06604807078838348, 0.0, 0.029783334583044052, 0.0]\n",
      "{'SA': [-0.727773904800415, -0.4257946312427521, -0.44172143936157227, -2.206166982650757, -0.3247111737728119], 'ADP': [-0.33288583159446716, -0.18266025185585022, -0.5938645005226135, -1.4301812648773193, -0.42584115266799927, -0.21135243773460388, -0.3698232173919678], 'DDDA': [], 'AZL': [], '1,4-CHDA': [-0.26026058197021484], '1,4-CHDA (95% trans)': [], '1,3-CHDA': [-1.7646515369415283, -2.20985746383667, -2.0962419509887695], '1,2-CHDA': [-0.3163483738899231, -2.2430825233459473, -2.1730713844299316], 'HHPA': [], 'PA': [-0.41691508889198303], 'IPA': [-0.5242019295692444, -0.5389451384544373, -0.6450991034507751, -0.44542402029037476, -0.8235787749290466, -0.49247175455093384, -0.2817649245262146, -0.9614717364311218, -1.1495639085769653, -0.08941234648227692, -0.6315709948539734, -0.3859187960624695, -0.487457275390625, -1.2390835285186768, -0.7864358425140381, -0.6918724179267883, -0.6894406080245972, -0.6960124373435974, -0.5215385556221008, -0.7079055905342102, -0.3421814441680908, -0.7128521203994751, -0.5171235799789429, -1.429985761642456, -0.7807940244674683, -0.45221805572509766, -0.8595544695854187, -0.5074048042297363, -0.9300839900970459, -1.29380464553833, -1.724608302116394, -1.26960027217865, -1.064122200012207, -0.3868486285209656, -0.931745171546936, -0.7034798264503479, -0.35292917490005493, -0.5987470149993896], 'TPA': [0.002120270626619458, 0.0004956808406859636, -1.479537844657898, 0.0019919294863939285, 0.004649707581847906, 0.0, -0.9277279376983643, 0.0025545505341142416, 0.0, 0.0017334733856841922, 0.005011791829019785, 0.0031809417996555567, 0.0027984536718577147, 0.002815201412886381, 0.0021094968542456627, 0.00286330608651042, 0.0028833141550421715, 0.0020916403736919165, 0.0031581223011016846, 0.0018291114829480648, 0.003476687939837575, 0.0020523294806480408, 0.003761963453143835, 0.003219206351786852, 0.005135225132107735, 0.0, 0.0037686845753341913, 0.0028454051353037357, 0.0], 'TMA': [-0.4810654819011688]}\n",
      "{'EG': [-0.16116763651371002, -0.2904195785522461], 'DEG': [-0.09748543053865433], '1,3-PROP': [-0.289745569229126, -0.12349295616149902], '1,4-BUT': [-0.18805375695228577], 'HDO': [], 'NPG': [-0.027665887027978897, -0.35939761996269226, -0.2836940884590149], '1,4-CHDM': [-0.037473827600479126, -0.35195523500442505, -0.016513895243406296, -0.21590568125247955, -0.4230416417121887, -0.2986268401145935, -0.24077792465686798, -0.03668108582496643, -0.015329865738749504, -0.017708338797092438, -0.03129512071609497, -0.08857887983322144, -0.05622025951743126, -0.04946015030145645, -0.049756135791540146, -0.03728344291448593, -0.050606317818164825, -0.03696782886981964, -0.24133965373039246, -0.055816952139139175, -0.061447322368621826, -0.06648929417133331, -0.09076045453548431, -0.07089881598949432, -0.02227655239403248, -0.025678757578134537, -0.2911989986896515, -0.017754942178726196], '1,3-CHDM': [-0.03545738384127617, -0.051324501633644104, -0.032559171319007874, -0.03653254732489586, -0.3184068202972412, -0.05064971745014191], 'TMCD': [-0.156107097864151, -0.261303573846817, -0.21108922362327576, -0.14821845293045044, -0.16409464180469513, -0.1864943504333496, -0.22635720670223236, -0.13780787587165833, -0.3689984977245331, -0.23420029878616333, -0.09112056344747543, -0.20603924989700317, -0.33427026867866516, -0.2072722166776657, -0.15531395375728607, -0.21081392467021942, -0.18060797452926636, -0.21454600989818573, -0.15399914979934692, -0.23252010345458984, -0.13610336184501648, -0.2559749484062195, -0.1527128368616104, -0.27697858214378357, -0.10272779315710068, -0.37808647751808167, -0.22734154760837555, -0.26982590556144714, -0.21172526478767395, -0.1704622060060501, -0.290298193693161], 'TMP': [-0.030375728383660316, -0.055578701198101044, -0.01876051351428032, -0.034094542264938354, -0.06680427491664886, -0.047157395631074905, 0.006469270214438438, -0.02011745050549507, -0.038857877254486084, -0.05414922162890434, -0.012219559401273727, -0.02519253082573414, -0.021513335406780243, -0.03374626487493515], 'MPD': [-0.023702267557382584, 0.0, 0.0, -0.019146256148815155, 0.0, 0.0, -0.15057504177093506, -0.030320916324853897, -0.0317583791911602, -0.35320940613746643, -0.004181359428912401, -0.004871455952525139, -0.0035575705114752054, -0.026332300156354904], 'TCDDM': []}\n"
     ]
    }
   ],
   "source": [
    "# Summarize importance scores:\n",
    "from polymerlearn.utils.graph_prep import get_AG_info\n",
    "\n",
    "# Mw summary:\n",
    "acid_scores = []\n",
    "glycol_scores = []\n",
    "\n",
    "mw_scores = []\n",
    "an_scores = []\n",
    "ohn_scores = []\n",
    "tmp_scores = []\n",
    "\n",
    "acid_names = pd.Series([c[1:] for c in data_mask.columns[20:33].tolist()])\n",
    "glycol_names = pd.Series([c[1:] for c in data_mask.columns[34:46].tolist()])\n",
    "acids, glycols, _, _ = get_AG_info(data_mask)\n",
    "\n",
    "acid_key = {a:[] for a in acid_names}\n",
    "glycol_key = {g:[] for g in glycol_names}\n",
    "\n",
    "for i in range(len(exp_summary)):\n",
    "\n",
    "    df_ind = exp_summary[i]['table_ind']\n",
    "\n",
    "    for a in range(len(acids[df_ind])):\n",
    "        acid_key[acids[df_ind][a]].append(exp_summary[i]['A'][a].item()) \n",
    "    \n",
    "    for g in range(len(glycols[df_ind])):\n",
    "        glycol_key[glycols[df_ind][g]].append(exp_summary[i]['G'][g].item()) \n",
    "\n",
    "    acid_scores.append(torch.sum(exp_summary[i]['A']).item())\n",
    "    glycol_scores.append(torch.sum(exp_summary[i]['G']).item())\n",
    "\n",
    "    # Break down individual scores:\n",
    "    mw_scores.append(exp_summary[i]['add'][0].item())\n",
    "    an_scores.append(exp_summary[i]['add'][1].item())\n",
    "    ohn_scores.append(exp_summary[i]['add'][2].item())\n",
    "    tmp_scores.append(exp_summary[i]['add'][3].item())\n",
    "\n",
    "print(acid_scores)\n",
    "print(glycol_scores)\n",
    "\n",
    "print(mw_scores)\n",
    "print(an_scores)\n",
    "print(ohn_scores)\n",
    "print(tmp_scores)\n",
    "\n",
    "print(acid_key)\n",
    "print(glycol_key)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
