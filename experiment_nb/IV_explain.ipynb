{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from polymerlearn.utils import get_IV_add, GraphDataset\n",
    "\n",
    "# Load data from local path:\n",
    "data = pd.read_csv(os.path.join('/Users/owenqueen/Desktop/eastman_project-confidential/Eastman_Project/CombinedData', \n",
    "            'pub_data.csv'))\n",
    "\n",
    "add = get_IV_add(data)\n",
    "\n",
    "dataset = GraphDataset(\n",
    "    data = data,\n",
    "    structure_dir = '../Structures/AG/xyz',\n",
    "    Y_target=['IV'],\n",
    "    test_size = 0.2,\n",
    "    add_features=add\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenqueen/Desktop/Eastman_project-Confidential/Eastman_Project/PolymerGNN/polymerlearn/utils/graph_prep.py:374: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1640811925055/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  return self.test_data, torch.tensor(self.Ytest).float(), torch.tensor(self.add_test).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, \t Train r2: -0.8997 \t Train Loss: 4.9989 \t Test r2: -0.1446 \t Test Loss 0.0437\n",
      "Epoch: 10, \t Train r2: 0.1496 \t Train Loss: 2.2337 \t Test r2: 0.4057 \t Test Loss 0.0227\n",
      "Epoch: 20, \t Train r2: 0.3836 \t Train Loss: 2.1899 \t Test r2: 0.5019 \t Test Loss 0.0190\n",
      "Epoch: 30, \t Train r2: 0.3142 \t Train Loss: 1.9694 \t Test r2: 0.5087 \t Test Loss 0.0188\n",
      "Epoch: 40, \t Train r2: 0.3454 \t Train Loss: 2.0457 \t Test r2: 0.5328 \t Test Loss 0.0178\n",
      "Epoch: 50, \t Train r2: 0.4531 \t Train Loss: 2.4713 \t Test r2: 0.5583 \t Test Loss 0.0169\n",
      "Epoch: 60, \t Train r2: 0.3992 \t Train Loss: 1.3919 \t Test r2: 0.5811 \t Test Loss 0.0160\n",
      "Epoch: 70, \t Train r2: 0.4733 \t Train Loss: 1.4838 \t Test r2: 0.5910 \t Test Loss 0.0156\n",
      "Epoch: 80, \t Train r2: 0.5291 \t Train Loss: 1.4367 \t Test r2: 0.6107 \t Test Loss 0.0149\n",
      "Epoch: 90, \t Train r2: 0.1988 \t Train Loss: 1.6848 \t Test r2: 0.6455 \t Test Loss 0.0135\n",
      "Epoch: 100, \t Train r2: 0.4306 \t Train Loss: 1.2140 \t Test r2: 0.6425 \t Test Loss 0.0136\n",
      "Epoch: 110, \t Train r2: 0.4484 \t Train Loss: 2.0861 \t Test r2: 0.5619 \t Test Loss 0.0167\n",
      "Epoch: 120, \t Train r2: 0.4387 \t Train Loss: 1.4896 \t Test r2: 0.6588 \t Test Loss 0.0130\n",
      "Epoch: 130, \t Train r2: 0.6961 \t Train Loss: 0.9189 \t Test r2: 0.6786 \t Test Loss 0.0123\n",
      "Epoch: 140, \t Train r2: 0.6885 \t Train Loss: 0.8131 \t Test r2: 0.6338 \t Test Loss 0.0140\n",
      "Epoch: 150, \t Train r2: 0.5414 \t Train Loss: 2.0357 \t Test r2: 0.6431 \t Test Loss 0.0136\n",
      "Epoch: 160, \t Train r2: 0.5302 \t Train Loss: 1.2363 \t Test r2: 0.6626 \t Test Loss 0.0129\n",
      "Epoch: 170, \t Train r2: 0.5723 \t Train Loss: 1.5644 \t Test r2: 0.6007 \t Test Loss 0.0152\n",
      "Epoch: 180, \t Train r2: 0.6840 \t Train Loss: 1.6898 \t Test r2: 0.6637 \t Test Loss 0.0128\n",
      "Epoch: 190, \t Train r2: 0.7064 \t Train Loss: 0.7214 \t Test r2: 0.5837 \t Test Loss 0.0159\n",
      "Epoch: 200, \t Train r2: 0.6623 \t Train Loss: 1.0464 \t Test r2: 0.6703 \t Test Loss 0.0126\n",
      "Epoch: 210, \t Train r2: 0.6775 \t Train Loss: 0.6388 \t Test r2: 0.7150 \t Test Loss 0.0109\n",
      "Epoch: 220, \t Train r2: 0.6094 \t Train Loss: 1.5407 \t Test r2: 0.7179 \t Test Loss 0.0108\n",
      "Epoch: 230, \t Train r2: 0.7150 \t Train Loss: 0.9093 \t Test r2: 0.7102 \t Test Loss 0.0111\n",
      "Epoch: 240, \t Train r2: 0.7989 \t Train Loss: 0.6510 \t Test r2: 0.5102 \t Test Loss 0.0187\n",
      "Epoch: 250, \t Train r2: 0.6521 \t Train Loss: 0.7372 \t Test r2: 0.7280 \t Test Loss 0.0104\n",
      "Epoch: 260, \t Train r2: 0.7473 \t Train Loss: 0.6024 \t Test r2: 0.6518 \t Test Loss 0.0133\n",
      "Epoch: 270, \t Train r2: 0.6602 \t Train Loss: 1.3412 \t Test r2: 0.6686 \t Test Loss 0.0127\n",
      "Epoch: 280, \t Train r2: 0.7272 \t Train Loss: 1.0658 \t Test r2: 0.5814 \t Test Loss 0.0160\n",
      "Epoch: 290, \t Train r2: 0.7245 \t Train Loss: 1.1966 \t Test r2: 0.5500 \t Test Loss 0.0172\n",
      "Epoch: 300, \t Train r2: 0.7487 \t Train Loss: 0.6332 \t Test r2: 0.7437 \t Test Loss 0.0098\n",
      "Epoch: 310, \t Train r2: 0.7784 \t Train Loss: 0.6039 \t Test r2: 0.4937 \t Test Loss 0.0193\n",
      "Epoch: 320, \t Train r2: 0.7951 \t Train Loss: 0.9436 \t Test r2: 0.5696 \t Test Loss 0.0164\n",
      "Epoch: 330, \t Train r2: 0.6604 \t Train Loss: 0.6104 \t Test r2: 0.7108 \t Test Loss 0.0110\n",
      "Epoch: 340, \t Train r2: 0.7415 \t Train Loss: 0.5669 \t Test r2: 0.7114 \t Test Loss 0.0110\n",
      "Epoch: 350, \t Train r2: 0.8783 \t Train Loss: 0.4037 \t Test r2: 0.6664 \t Test Loss 0.0127\n",
      "Epoch: 360, \t Train r2: 0.5654 \t Train Loss: 1.4884 \t Test r2: 0.6596 \t Test Loss 0.0130\n",
      "Epoch: 370, \t Train r2: 0.7538 \t Train Loss: 1.0238 \t Test r2: 0.6866 \t Test Loss 0.0120\n",
      "Epoch: 380, \t Train r2: 0.8588 \t Train Loss: 0.4073 \t Test r2: 0.6547 \t Test Loss 0.0132\n",
      "Epoch: 390, \t Train r2: 0.7808 \t Train Loss: 1.0786 \t Test r2: 0.6262 \t Test Loss 0.0143\n",
      "Epoch: 400, \t Train r2: 0.7682 \t Train Loss: 0.6267 \t Test r2: 0.7577 \t Test Loss 0.0092\n",
      "Epoch: 410, \t Train r2: 0.8062 \t Train Loss: 0.6950 \t Test r2: 0.4852 \t Test Loss 0.0197\n",
      "Epoch: 420, \t Train r2: 0.7409 \t Train Loss: 1.0376 \t Test r2: 0.7136 \t Test Loss 0.0109\n",
      "Epoch: 430, \t Train r2: 0.8816 \t Train Loss: 0.3500 \t Test r2: 0.7221 \t Test Loss 0.0106\n",
      "Epoch: 440, \t Train r2: 0.8648 \t Train Loss: 0.5771 \t Test r2: 0.4708 \t Test Loss 0.0202\n",
      "Epoch: 450, \t Train r2: 0.8424 \t Train Loss: 0.4214 \t Test r2: 0.7605 \t Test Loss 0.0091\n",
      "Epoch: 460, \t Train r2: 0.7892 \t Train Loss: 0.6178 \t Test r2: 0.7484 \t Test Loss 0.0096\n",
      "Epoch: 470, \t Train r2: 0.7038 \t Train Loss: 0.8081 \t Test r2: 0.6829 \t Test Loss 0.0121\n",
      "Epoch: 480, \t Train r2: 0.8024 \t Train Loss: 0.4535 \t Test r2: 0.7320 \t Test Loss 0.0102\n",
      "Epoch: 490, \t Train r2: 0.8825 \t Train Loss: 0.4300 \t Test r2: 0.7253 \t Test Loss 0.0105\n",
      "Epoch: 500, \t Train r2: 0.8462 \t Train Loss: 0.4189 \t Test r2: 0.8089 \t Test Loss 0.0073\n",
      "Epoch: 510, \t Train r2: 0.8173 \t Train Loss: 0.4710 \t Test r2: 0.7792 \t Test Loss 0.0084\n",
      "Epoch: 520, \t Train r2: 0.8076 \t Train Loss: 0.7026 \t Test r2: 0.6543 \t Test Loss 0.0132\n",
      "Epoch: 530, \t Train r2: 0.9257 \t Train Loss: 0.2382 \t Test r2: 0.7074 \t Test Loss 0.0112\n",
      "Epoch: 540, \t Train r2: 0.7928 \t Train Loss: 0.8153 \t Test r2: 0.7406 \t Test Loss 0.0099\n",
      "Epoch: 550, \t Train r2: 0.8263 \t Train Loss: 0.7035 \t Test r2: 0.5958 \t Test Loss 0.0154\n",
      "Epoch: 560, \t Train r2: 0.8750 \t Train Loss: 0.2692 \t Test r2: 0.6708 \t Test Loss 0.0126\n",
      "Epoch: 570, \t Train r2: 0.8500 \t Train Loss: 0.3026 \t Test r2: 0.7225 \t Test Loss 0.0106\n",
      "Epoch: 580, \t Train r2: 0.8204 \t Train Loss: 0.8631 \t Test r2: 0.5755 \t Test Loss 0.0162\n",
      "Epoch: 590, \t Train r2: 0.8282 \t Train Loss: 0.6757 \t Test r2: 0.7025 \t Test Loss 0.0114\n",
      "Epoch: 600, \t Train r2: 0.8653 \t Train Loss: 0.2770 \t Test r2: 0.7107 \t Test Loss 0.0110\n",
      "Epoch: 610, \t Train r2: 0.8915 \t Train Loss: 0.2796 \t Test r2: 0.7216 \t Test Loss 0.0106\n",
      "Epoch: 620, \t Train r2: 0.8212 \t Train Loss: 0.8133 \t Test r2: 0.6394 \t Test Loss 0.0138\n",
      "Epoch: 630, \t Train r2: 0.8247 \t Train Loss: 0.4655 \t Test r2: 0.6761 \t Test Loss 0.0124\n",
      "Epoch: 640, \t Train r2: 0.8515 \t Train Loss: 0.2570 \t Test r2: 0.7447 \t Test Loss 0.0097\n",
      "Epoch: 650, \t Train r2: 0.5987 \t Train Loss: 0.7678 \t Test r2: 0.6917 \t Test Loss 0.0118\n",
      "Epoch: 660, \t Train r2: 0.8384 \t Train Loss: 0.7148 \t Test r2: 0.5827 \t Test Loss 0.0159\n",
      "Epoch: 670, \t Train r2: 0.8647 \t Train Loss: 0.3294 \t Test r2: 0.7344 \t Test Loss 0.0101\n",
      "Epoch: 680, \t Train r2: 0.8589 \t Train Loss: 0.3457 \t Test r2: 0.7510 \t Test Loss 0.0095\n",
      "Epoch: 690, \t Train r2: 0.3963 \t Train Loss: 2.1567 \t Test r2: 0.7542 \t Test Loss 0.0094\n",
      "Epoch: 700, \t Train r2: 0.8813 \t Train Loss: 0.5042 \t Test r2: 0.6385 \t Test Loss 0.0138\n",
      "Epoch: 710, \t Train r2: 0.8565 \t Train Loss: 0.6172 \t Test r2: 0.7096 \t Test Loss 0.0111\n",
      "Epoch: 720, \t Train r2: 0.8732 \t Train Loss: 0.2922 \t Test r2: 0.6900 \t Test Loss 0.0118\n",
      "Epoch: 730, \t Train r2: 0.7810 \t Train Loss: 0.5808 \t Test r2: 0.7141 \t Test Loss 0.0109\n",
      "Epoch: 740, \t Train r2: 0.8245 \t Train Loss: 0.4900 \t Test r2: 0.6426 \t Test Loss 0.0136\n",
      "Epoch: 750, \t Train r2: 0.8283 \t Train Loss: 0.4954 \t Test r2: 0.7222 \t Test Loss 0.0106\n",
      "Epoch: 760, \t Train r2: 0.8095 \t Train Loss: 0.8214 \t Test r2: 0.6610 \t Test Loss 0.0129\n",
      "Epoch: 770, \t Train r2: 0.8666 \t Train Loss: 0.3421 \t Test r2: 0.7397 \t Test Loss 0.0099\n",
      "Epoch: 780, \t Train r2: 0.8272 \t Train Loss: 0.4337 \t Test r2: 0.7844 \t Test Loss 0.0082\n",
      "Epoch: 790, \t Train r2: 0.8920 \t Train Loss: 0.2502 \t Test r2: 0.7464 \t Test Loss 0.0097\n"
     ]
    }
   ],
   "source": [
    "from polymerlearn.models.gnn import PolymerGNN_IV\n",
    "from polymerlearn.utils import train\n",
    "\n",
    "model_kwargs = {\n",
    "    'input_feat': 6,         # How many input features on each node; don't change this\n",
    "    'hidden_channels': 32,   # How many intermediate dimensions to use in model\n",
    "                            # Can change this ^^\n",
    "    'num_additional': 4      # How many additional resin properties to include in the prediction\n",
    "                            # Corresponds to the number in get_IV_add\n",
    "}\n",
    "\n",
    "model = PolymerGNN_IV(**model_kwargs)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    optimizer = optimizer,\n",
    "    criterion = criterion,\n",
    "    dataset = dataset,\n",
    "    batch_size = 64,\n",
    "    epochs = 800\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenqueen/Desktop/Eastman_project-Confidential/Eastman_Project/PolymerGNN/polymerlearn/explain/explain_gnn.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(add_test).float())\n"
     ]
    }
   ],
   "source": [
    "from polymerlearn.explain import PolymerGNN_IV_EXPLAIN, PolymerGNNExplainer\n",
    "\n",
    "mexplain = PolymerGNN_IV_EXPLAIN(**model_kwargs)\n",
    "mexplain.load_state_dict(model.state_dict()) # Load weights from trained model over to explaining one\n",
    "\n",
    "explainer = PolymerGNNExplainer(mexplain)\n",
    "\n",
    "test_batch, Ytest, add_test = dataset.get_test()\n",
    "test_inds = dataset.test_mask\n",
    "\n",
    "exp_summary = []\n",
    "\n",
    "for i in range(Ytest.shape[0]):\n",
    "    scores = explainer.get_explanation(test_batch[i], add_test[i])\n",
    "    scores['A'] = torch.sum(scores['A'], dim = 1)\n",
    "    scores['G'] = torch.sum(scores['G'], dim = 1)\n",
    "    scores['table_ind'] = test_inds[i]\n",
    "\n",
    "    exp_summary.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 23976.0 \t log(Table): 10.084808608996498 \t Add test: 10.084808349609375\n",
      "Table: 94705.0 \t log(Table): 11.458522076090842 \t Add test: 11.458521842956543\n",
      "Table: 24619.0 \t log(Table): 10.111273781529295 \t Add test: 10.111273765563965\n",
      "Table: 47381.0 \t log(Table): 10.765976583441928 \t Add test: 10.765976905822754\n",
      "Table: 24860.0 \t log(Table): 10.121015365064702 \t Add test: 10.121015548706055\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 14098.0 \t log(Table): 9.553788222333822 \t Add test: 9.553788185119629\n",
      "Table: 68358.0 \t log(Table): 11.13251387992617 \t Add test: 11.132513999938965\n",
      "Table: 97437.0 \t log(Table): 11.486961294292419 \t Add test: 11.486961364746094\n",
      "Table: 56224.0 \t log(Table): 10.937098990986824 \t Add test: 10.93709945678711\n",
      "Table: 52715.0 \t log(Table): 10.87265532401105 \t Add test: 10.872654914855957\n",
      "Table: 38227.0 \t log(Table): 10.551297351207467 \t Add test: 10.551297187805176\n",
      "Table: 27730.0 \t log(Table): 10.230270138609823 \t Add test: 10.230270385742188\n",
      "Table: 17842.0 \t log(Table): 9.78931050747373 \t Add test: 9.789310455322266\n",
      "Table: 19288.0 \t log(Table): 9.867238419861007 \t Add test: 9.86723804473877\n",
      "Table: 28564.0 \t log(Table): 10.259902462669 \t Add test: 10.259902000427246\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 118232.0 \t log(Table): 11.680404074894408 \t Add test: 11.680403709411621\n",
      "Table: 1708.0 \t log(Table): 7.443078374348516 \t Add test: 7.443078517913818\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 38824.0 \t log(Table): 10.566793891085052 \t Add test: 10.566793441772461\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 16310.0 \t log(Table): 9.69953369561506 \t Add test: 9.699533462524414\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 49542.0 \t log(Table): 10.810576073645484 \t Add test: 10.810576438903809\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 7592.0 \t log(Table): 8.934850340289763 \t Add test: 8.934850692749023\n",
      "Table: 112649.0 \t log(Table): 11.63203206876817 \t Add test: 11.63203239440918\n",
      "Table: 82050.0 \t log(Table): 11.315084096518238 \t Add test: 11.315084457397461\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 33155.0 \t log(Table): 10.408948813802969 \t Add test: 10.40894889831543\n",
      "Table: 58632.0 \t log(Table): 10.979035901605686 \t Add test: 10.979036331176758\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 20705.0 \t log(Table): 9.938130495979667 \t Add test: 9.938130378723145\n",
      "Table: 42089.0 \t log(Table): 10.647541702869878 \t Add test: 10.647541999816895\n",
      "Table: 131365.0 \t log(Table): 11.785734987299259 \t Add test: 11.785735130310059\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 86128.0 \t log(Table): 11.363589840800739 \t Add test: 11.363590240478516\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_mask = data.loc[data['IV'].notna(),:]\n",
    "\n",
    "for i in range(len(test_inds)):\n",
    "    j = test_inds[i]\n",
    "    mw_table = data_mask['Mw (PS)'].iloc[j]\n",
    "    # Test mw from add_test\n",
    "    mw_add_test = add_test[i,0]\n",
    "\n",
    "    print('Table: {} \\t log(Table): {} \\t Add test: {}'.format(\n",
    "        mw_table, np.log(mw_table), mw_add_test.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5220816731452942, -0.871830940246582, -1.372873067855835, -0.7051889300346375, -1.479537844657898, -0.8235787749290466, -0.49047982692718506, -0.4644251763820648, -1.7646515369415283, -1.5553362369537354, -1.144914150238037, -2.20985746383667, -1.4301812648773193, -0.5704778432846069, -0.9277279376983643, -0.6290164589881897, -0.811713457107544, -0.4857237935066223, -1.2340717315673828, -0.7832549214363098, -2.0962419509887695, -0.3163483738899231, -0.6890739798545837, -1.1152818202972412, -0.6931972503662109, -0.519429087638855, -0.70504230260849, -0.5535339117050171, -0.7099688053131104, -0.5150319337844849, -1.429985761642456, -0.777635931968689, -0.45038893818855286, -0.8560777902603149, -0.5053524971008301, -0.9263220429420471, -0.41691508889198303, -1.2905853986740112, -1.724608302116394, -1.264465093612671, -1.064122200012207, -0.8285700678825378, -0.9279764890670776, -0.7006344199180603, -2.206166982650757, -0.6776403188705444, -2.2430825233459473, -0.9685702323913574, -2.1730713844299316]\n",
      "[-0.19358092546463013, -0.31538155674934387, -0.4075339436531067, -0.24636363983154297, -0.16116763651371002, -0.2500002384185791, -0.1836758404970169, -0.1832408905029297, -0.289745569229126, -0.4898459315299988, -0.3457842469215393, -0.09748543053865433, -0.24077792465686798, -0.1872561275959015, -0.12349295616149902, -0.22302083671092987, -0.26418301463127136, -0.1691029965877533, -0.4575773775577545, -0.2904205620288849, -0.35939761996269226, -0.09112056344747543, -0.25549939274787903, -0.40344905853271484, -0.25702834129333496, -0.1925973892211914, -0.26142024993896484, -0.21236635744571686, -0.26587051153182983, -0.19096697866916656, -0.24133965373039246, -0.2883370518684387, -0.16866253316402435, -0.3174222707748413, -0.18924538791179657, -0.3434678912162781, -0.10272779315710068, -0.37255603075027466, -0.2836940884590149, -0.4688469171524048, -0.42410820722579956, -0.2660190165042877, -0.32556864619255066, -0.26237499713897705, -0.2911989986896515, -0.2132880538702011, -0.2904195785522461, -0.35037675499916077, -0.18805375695228577]\n",
      "[0.7059991955757141, 1.1890240907669067, 2.005154609680176, 0.9321068525314331, 2.0319747924804688, 1.0864759683609009, 0.6632646918296814, 0.5958696007728577, 2.648825168609619, 2.4161291122436523, 1.6239137649536133, 3.260350465774536, 2.0193707942962646, 0.6346941590309143, 1.166871428489685, 0.8049064874649048, 1.0142182111740112, 0.6417993307113647, 1.668807029724121, 1.0591778755187988, 3.3014161586761475, 0.30126750469207764, 0.931818962097168, 1.5937443971633911, 0.9373953342437744, 0.7024122476577759, 0.9534127116203308, 0.7260822057723999, 0.9600745439529419, 0.6964661478996277, 1.9998971223831177, 1.051579475402832, 0.6090506911277771, 1.1576547622680664, 0.683376669883728, 1.2526445388793945, 0.4081835150718689, 1.9697016477584839, 2.5244998931884766, 1.7099077701568604, 1.4329311847686768, 1.0910775661468506, 1.2548816204071045, 0.9474520683288574, 2.2576370239257812, 0.8280795812606812, 3.4275338649749756, 1.365840196609497, 3.201616048812866]\n",
      "[-0.07748078554868698, 0.010476435534656048, -0.1635705828666687, -0.06612090021371841, -0.10808213800191879, -0.1267797350883484, -0.07865428924560547, -0.056069545447826385, -0.1552579402923584, -0.15307149291038513, -0.13032180070877075, -0.20338153839111328, -0.11930190771818161, -0.03776582330465317, -0.08706940710544586, -0.06983032077550888, -0.0023000070359557867, -0.09305182099342346, 0.02722134068608284, -0.06517401337623596, -0.1844320446252823, -0.025231139734387398, -0.07017552107572556, 0.025654099881649017, -0.0649535059928894, -0.09067077189683914, -0.05386967957019806, -0.025099165737628937, -0.07461056858301163, -0.09324397146701813, -0.12071261554956436, -0.11869584769010544, -0.07948336005210876, -0.050791021436452866, -0.08882081508636475, -0.11898224800825119, -0.027310915291309357, -0.12439130246639252, -0.14558321237564087, 0.004838619381189346, -0.11159058660268784, -0.04421091452240944, -0.09570912271738052, -0.07266847789287567, 0.0022639771923422813, -0.04577193036675453, -0.18976622819900513, -0.10648562014102936, -0.183843195438385]\n",
      "[-0.23808974027633667, -0.42200836539268494, -0.4902564287185669, -0.2829187214374542, -0.6030522584915161, -0.2798883318901062, -0.22754992544651031, -0.20630736649036407, -0.7235944867134094, -0.6330674886703491, -0.44295573234558105, -0.9114898443222046, -0.5769447684288025, -0.15681029856204987, -0.4143129885196686, -0.2408953458070755, -0.312211811542511, -0.20586660504341125, -0.37285614013671875, -0.28846275806427, -0.8595629334449768, -0.12201797962188721, -0.2742249667644501, -0.4859367311000824, -0.27994847297668457, -0.224294051527977, -0.29242148995399475, -0.27281540632247925, -0.26961275935173035, -0.2207055687904358, -0.5625925064086914, -0.23451504111289978, -0.21617382764816284, -0.3070274591445923, -0.22097621858119965, -0.24065172672271729, -0.1367741823196411, -0.5434624552726746, -0.6785044074058533, -0.34738561511039734, -0.21381597220897675, -0.3714749217033386, -0.29613742232322693, -0.2705255448818207, -0.05690170079469681, -0.3021281063556671, -0.8844234347343445, -0.348070353269577, -0.8568188548088074]\n",
      "[0.0, 0.03351076692342758, 0.07162146270275116, 0.008384385146200657, 0.0, 0.021967964246869087, 0.0, 0.0, 0.0, 0.09804382920265198, 0.06583348661661148, 0.0, 0.0, 0.0, 0.0, 0.010201496072113514, 0.010114677250385284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012003138661384583, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.006530595477670431, 0.0, 0.0, 0.0, 0.04795505106449127, 0.013681893236935139, 0.0, 0.0, 0.06604807078838348, 0.0, 0.029783334583044052, 0.0]\n",
      "{'SA': [-0.727773904800415, -0.4257946312427521, -0.44172143936157227, -2.206166982650757, -0.3247111737728119], 'ADP': [-0.33288583159446716, -0.18266025185585022, -0.5938645005226135, -1.4301812648773193, -0.42584115266799927, -0.21135243773460388, -0.3698232173919678], 'DDDA': [], 'AZL': [], '1,4-CHDA': [-0.26026058197021484], '1,4-CHDA (95% trans)': [], '1,3-CHDA': [-1.7646515369415283, -2.20985746383667, -2.0962419509887695], '1,2-CHDA': [-0.3163483738899231, -2.2430825233459473, -2.1730713844299316], 'HHPA': [], 'PA': [-0.41691508889198303], 'IPA': [-0.5242019295692444, -0.5389451384544373, -0.6450991034507751, -0.44542402029037476, -0.8235787749290466, -0.49247175455093384, -0.2817649245262146, -0.9614717364311218, -1.1495639085769653, -0.08941234648227692, -0.6315709948539734, -0.3859187960624695, -0.487457275390625, -1.2390835285186768, -0.7864358425140381, -0.6918724179267883, -0.6894406080245972, -0.6960124373435974, -0.5215385556221008, -0.7079055905342102, -0.3421814441680908, -0.7128521203994751, -0.5171235799789429, -1.429985761642456, -0.7807940244674683, -0.45221805572509766, -0.8595544695854187, -0.5074048042297363, -0.9300839900970459, -1.29380464553833, -1.724608302116394, -1.26960027217865, -1.064122200012207, -0.3868486285209656, -0.931745171546936, -0.7034798264503479, -0.35292917490005493, -0.5987470149993896], 'TPA': [0.002120270626619458, 0.0004956808406859636, -1.479537844657898, 0.0019919294863939285, 0.004649707581847906, 0.0, -0.9277279376983643, 0.0025545505341142416, 0.0, 0.0017334733856841922, 0.005011791829019785, 0.0031809417996555567, 0.0027984536718577147, 0.002815201412886381, 0.0021094968542456627, 0.00286330608651042, 0.0028833141550421715, 0.0020916403736919165, 0.0031581223011016846, 0.0018291114829480648, 0.003476687939837575, 0.0020523294806480408, 0.003761963453143835, 0.003219206351786852, 0.005135225132107735, 0.0, 0.0037686845753341913, 0.0028454051353037357, 0.0], 'TMA': [-0.4810654819011688]}\n",
      "{'EG': [-0.16116763651371002, -0.2904195785522461], 'DEG': [-0.09748543053865433], '1,3-PROP': [-0.289745569229126, -0.12349295616149902], '1,4-BUT': [-0.18805375695228577], 'HDO': [], 'NPG': [-0.027665887027978897, -0.35939761996269226, -0.2836940884590149], '1,4-CHDM': [-0.037473827600479126, -0.35195523500442505, -0.016513895243406296, -0.21590568125247955, -0.4230416417121887, -0.2986268401145935, -0.24077792465686798, -0.03668108582496643, -0.015329865738749504, -0.017708338797092438, -0.03129512071609497, -0.08857887983322144, -0.05622025951743126, -0.04946015030145645, -0.049756135791540146, -0.03728344291448593, -0.050606317818164825, -0.03696782886981964, -0.24133965373039246, -0.055816952139139175, -0.061447322368621826, -0.06648929417133331, -0.09076045453548431, -0.07089881598949432, -0.02227655239403248, -0.025678757578134537, -0.2911989986896515, -0.017754942178726196], '1,3-CHDM': [-0.03545738384127617, -0.051324501633644104, -0.032559171319007874, -0.03653254732489586, -0.3184068202972412, -0.05064971745014191], 'TMCD': [-0.156107097864151, -0.261303573846817, -0.21108922362327576, -0.14821845293045044, -0.16409464180469513, -0.1864943504333496, -0.22635720670223236, -0.13780787587165833, -0.3689984977245331, -0.23420029878616333, -0.09112056344747543, -0.20603924989700317, -0.33427026867866516, -0.2072722166776657, -0.15531395375728607, -0.21081392467021942, -0.18060797452926636, -0.21454600989818573, -0.15399914979934692, -0.23252010345458984, -0.13610336184501648, -0.2559749484062195, -0.1527128368616104, -0.27697858214378357, -0.10272779315710068, -0.37808647751808167, -0.22734154760837555, -0.26982590556144714, -0.21172526478767395, -0.1704622060060501, -0.290298193693161], 'TMP': [-0.030375728383660316, -0.055578701198101044, -0.01876051351428032, -0.034094542264938354, -0.06680427491664886, -0.047157395631074905, 0.006469270214438438, -0.02011745050549507, -0.038857877254486084, -0.05414922162890434, -0.012219559401273727, -0.02519253082573414, -0.021513335406780243, -0.03374626487493515], 'MPD': [-0.023702267557382584, 0.0, 0.0, -0.019146256148815155, 0.0, 0.0, -0.15057504177093506, -0.030320916324853897, -0.0317583791911602, -0.35320940613746643, -0.004181359428912401, -0.004871455952525139, -0.0035575705114752054, -0.026332300156354904], 'TCDDM': []}\n"
     ]
    }
   ],
   "source": [
    "# Summarize importance scores:\n",
    "\n",
    "# Mw summary:\n",
    "acid_scores = []\n",
    "glycol_scores = []\n",
    "\n",
    "mw_scores = []\n",
    "an_scores = []\n",
    "ohn_scores = []\n",
    "tmp_scores = []\n",
    "\n",
    "def get_AG_info(data, ac = (20,33), gc = (34,46)):\n",
    "\n",
    "    # Decompose the data into included names\n",
    "    acid_names = pd.Series([c[1:] for c in data.columns[ac[0]:ac[1]].tolist()])\n",
    "    glycol_names = pd.Series([c[1:] for c in data.columns[gc[0]:gc[1]].tolist()])\n",
    "\n",
    "    # Holds all names of acids and glycols\n",
    "    acid_included = []\n",
    "    glycol_included = []\n",
    "\n",
    "    # Keep track of percents in each acid, glycol\n",
    "    acid_pcts = []\n",
    "    glycol_pcts = []\n",
    "\n",
    "    # Get relevant names and percentages of acid/glycols\n",
    "    for i in range(data.shape[0]):\n",
    "\n",
    "        acid_hit = (data.iloc[i,ac[0]:ac[1]].to_numpy() > 0)\n",
    "        glycol_hit = (data.iloc[i,gc[0]:gc[1]].to_numpy() > 0)\n",
    "\n",
    "        # Add to percentage lists:\n",
    "        acid_pcts.append(data.iloc[i,ac[0]:ac[1]][acid_hit].tolist())\n",
    "        glycol_pcts.append(data.iloc[i,gc[0]:gc[1]][glycol_hit].tolist()) \n",
    "\n",
    "        acid_pos = acid_names[np.argwhere(acid_hit).flatten()].tolist()\n",
    "        glycol_pos = glycol_names[np.argwhere(glycol_hit).flatten()].tolist()\n",
    "\n",
    "        acid_included.append(acid_pos)\n",
    "        glycol_included.append(glycol_pos)\n",
    "\n",
    "    return acid_included, glycol_included, acid_pcts, glycol_pcts\n",
    "\n",
    "acid_names = pd.Series([c[1:] for c in data_mask.columns[20:33].tolist()])\n",
    "glycol_names = pd.Series([c[1:] for c in data_mask.columns[34:46].tolist()])\n",
    "acids, glycols, _, _ = get_AG_info(data_mask)\n",
    "\n",
    "acid_key = {a:[] for a in acid_names}\n",
    "glycol_key = {g:[] for g in glycol_names}\n",
    "\n",
    "for i in range(len(exp_summary)):\n",
    "\n",
    "    df_ind = exp_summary[i]['table_ind']\n",
    "\n",
    "    for a in range(len(acids[df_ind])):\n",
    "        acid_key[acids[df_ind][a]].append(exp_summary[i]['A'][a].item()) \n",
    "    \n",
    "    for g in range(len(glycols[df_ind])):\n",
    "        glycol_key[glycols[df_ind][g]].append(exp_summary[i]['G'][g].item()) \n",
    "\n",
    "    acid_scores.append(torch.sum(exp_summary[i]['A']).item())\n",
    "    glycol_scores.append(torch.sum(exp_summary[i]['G']).item())\n",
    "\n",
    "    mw_scores.append(exp_summary[i]['add'][0].item())\n",
    "    an_scores.append(exp_summary[i]['add'][1].item())\n",
    "    ohn_scores.append(exp_summary[i]['add'][2].item())\n",
    "    tmp_scores.append(exp_summary[i]['add'][3].item())\n",
    "\n",
    "print(acid_scores)\n",
    "print(glycol_scores)\n",
    "\n",
    "print(mw_scores)\n",
    "print(an_scores)\n",
    "print(ohn_scores)\n",
    "print(tmp_scores)\n",
    "\n",
    "print(acid_key)\n",
    "print(glycol_key)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
