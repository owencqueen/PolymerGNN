{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from polymerlearn.utils import get_IV_add, GraphDataset\n",
    "\n",
    "# Load data from local path:\n",
    "data = pd.read_csv(os.path.join('/Users/owenqueen/Desktop/eastman_project-confidential/Eastman_Project/CombinedData', \n",
    "            'pub_data.csv'))\n",
    "\n",
    "add = get_IV_add(data)\n",
    "\n",
    "dataset = GraphDataset(\n",
    "    data = data,\n",
    "    structure_dir = '../Structures/AG/xyz',\n",
    "    Y_target=['IV'],\n",
    "    test_size = 0.2,\n",
    "    add_features=add\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenqueen/Desktop/Eastman_project-Confidential/Eastman_Project/PolymerGNN/polymerlearn/utils/graph_prep.py:374: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1640811925055/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  return self.test_data, torch.tensor(self.Ytest).float(), torch.tensor(self.add_test).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, \t Train r2: -0.8997 \t Train Loss: 4.9989 \t Test r2: -0.1446 \t Test Loss 0.0437\n",
      "Epoch: 10, \t Train r2: 0.1496 \t Train Loss: 2.2337 \t Test r2: 0.4057 \t Test Loss 0.0227\n",
      "Epoch: 20, \t Train r2: 0.3836 \t Train Loss: 2.1899 \t Test r2: 0.5019 \t Test Loss 0.0190\n",
      "Epoch: 30, \t Train r2: 0.3142 \t Train Loss: 1.9694 \t Test r2: 0.5087 \t Test Loss 0.0188\n",
      "Epoch: 40, \t Train r2: 0.3454 \t Train Loss: 2.0457 \t Test r2: 0.5328 \t Test Loss 0.0178\n",
      "Epoch: 50, \t Train r2: 0.4531 \t Train Loss: 2.4713 \t Test r2: 0.5583 \t Test Loss 0.0169\n",
      "Epoch: 60, \t Train r2: 0.3992 \t Train Loss: 1.3919 \t Test r2: 0.5811 \t Test Loss 0.0160\n",
      "Epoch: 70, \t Train r2: 0.4733 \t Train Loss: 1.4838 \t Test r2: 0.5910 \t Test Loss 0.0156\n",
      "Epoch: 80, \t Train r2: 0.5291 \t Train Loss: 1.4367 \t Test r2: 0.6107 \t Test Loss 0.0149\n",
      "Epoch: 90, \t Train r2: 0.1988 \t Train Loss: 1.6848 \t Test r2: 0.6455 \t Test Loss 0.0135\n",
      "Epoch: 100, \t Train r2: 0.4306 \t Train Loss: 1.2140 \t Test r2: 0.6425 \t Test Loss 0.0136\n",
      "Epoch: 110, \t Train r2: 0.4484 \t Train Loss: 2.0861 \t Test r2: 0.5619 \t Test Loss 0.0167\n",
      "Epoch: 120, \t Train r2: 0.4387 \t Train Loss: 1.4896 \t Test r2: 0.6588 \t Test Loss 0.0130\n",
      "Epoch: 130, \t Train r2: 0.6961 \t Train Loss: 0.9189 \t Test r2: 0.6786 \t Test Loss 0.0123\n",
      "Epoch: 140, \t Train r2: 0.6885 \t Train Loss: 0.8131 \t Test r2: 0.6338 \t Test Loss 0.0140\n",
      "Epoch: 150, \t Train r2: 0.5414 \t Train Loss: 2.0357 \t Test r2: 0.6431 \t Test Loss 0.0136\n",
      "Epoch: 160, \t Train r2: 0.5302 \t Train Loss: 1.2363 \t Test r2: 0.6626 \t Test Loss 0.0129\n",
      "Epoch: 170, \t Train r2: 0.5723 \t Train Loss: 1.5644 \t Test r2: 0.6007 \t Test Loss 0.0152\n",
      "Epoch: 180, \t Train r2: 0.6840 \t Train Loss: 1.6898 \t Test r2: 0.6637 \t Test Loss 0.0128\n",
      "Epoch: 190, \t Train r2: 0.7064 \t Train Loss: 0.7214 \t Test r2: 0.5837 \t Test Loss 0.0159\n",
      "Epoch: 200, \t Train r2: 0.6623 \t Train Loss: 1.0464 \t Test r2: 0.6703 \t Test Loss 0.0126\n",
      "Epoch: 210, \t Train r2: 0.6775 \t Train Loss: 0.6388 \t Test r2: 0.7150 \t Test Loss 0.0109\n",
      "Epoch: 220, \t Train r2: 0.6094 \t Train Loss: 1.5407 \t Test r2: 0.7179 \t Test Loss 0.0108\n",
      "Epoch: 230, \t Train r2: 0.7150 \t Train Loss: 0.9093 \t Test r2: 0.7102 \t Test Loss 0.0111\n",
      "Epoch: 240, \t Train r2: 0.7989 \t Train Loss: 0.6510 \t Test r2: 0.5102 \t Test Loss 0.0187\n",
      "Epoch: 250, \t Train r2: 0.6521 \t Train Loss: 0.7372 \t Test r2: 0.7280 \t Test Loss 0.0104\n",
      "Epoch: 260, \t Train r2: 0.7473 \t Train Loss: 0.6024 \t Test r2: 0.6518 \t Test Loss 0.0133\n",
      "Epoch: 270, \t Train r2: 0.6602 \t Train Loss: 1.3412 \t Test r2: 0.6686 \t Test Loss 0.0127\n",
      "Epoch: 280, \t Train r2: 0.7272 \t Train Loss: 1.0658 \t Test r2: 0.5814 \t Test Loss 0.0160\n",
      "Epoch: 290, \t Train r2: 0.7245 \t Train Loss: 1.1966 \t Test r2: 0.5500 \t Test Loss 0.0172\n",
      "Epoch: 300, \t Train r2: 0.7487 \t Train Loss: 0.6332 \t Test r2: 0.7437 \t Test Loss 0.0098\n",
      "Epoch: 310, \t Train r2: 0.7784 \t Train Loss: 0.6039 \t Test r2: 0.4937 \t Test Loss 0.0193\n",
      "Epoch: 320, \t Train r2: 0.7951 \t Train Loss: 0.9436 \t Test r2: 0.5696 \t Test Loss 0.0164\n",
      "Epoch: 330, \t Train r2: 0.6604 \t Train Loss: 0.6104 \t Test r2: 0.7108 \t Test Loss 0.0110\n",
      "Epoch: 340, \t Train r2: 0.7415 \t Train Loss: 0.5669 \t Test r2: 0.7114 \t Test Loss 0.0110\n",
      "Epoch: 350, \t Train r2: 0.8783 \t Train Loss: 0.4037 \t Test r2: 0.6664 \t Test Loss 0.0127\n",
      "Epoch: 360, \t Train r2: 0.5654 \t Train Loss: 1.4884 \t Test r2: 0.6596 \t Test Loss 0.0130\n",
      "Epoch: 370, \t Train r2: 0.7538 \t Train Loss: 1.0238 \t Test r2: 0.6866 \t Test Loss 0.0120\n",
      "Epoch: 380, \t Train r2: 0.8588 \t Train Loss: 0.4073 \t Test r2: 0.6547 \t Test Loss 0.0132\n",
      "Epoch: 390, \t Train r2: 0.7808 \t Train Loss: 1.0786 \t Test r2: 0.6262 \t Test Loss 0.0143\n",
      "Epoch: 400, \t Train r2: 0.7682 \t Train Loss: 0.6267 \t Test r2: 0.7577 \t Test Loss 0.0092\n",
      "Epoch: 410, \t Train r2: 0.8062 \t Train Loss: 0.6950 \t Test r2: 0.4852 \t Test Loss 0.0197\n",
      "Epoch: 420, \t Train r2: 0.7409 \t Train Loss: 1.0376 \t Test r2: 0.7136 \t Test Loss 0.0109\n",
      "Epoch: 430, \t Train r2: 0.8816 \t Train Loss: 0.3500 \t Test r2: 0.7221 \t Test Loss 0.0106\n",
      "Epoch: 440, \t Train r2: 0.8648 \t Train Loss: 0.5771 \t Test r2: 0.4708 \t Test Loss 0.0202\n"
     ]
    }
   ],
   "source": [
    "from polymerlearn.models.gnn import PolymerGNN_IV\n",
    "from polymerlearn.utils import train\n",
    "\n",
    "model_kwargs = {\n",
    "    'input_feat': 6,         # How many input features on each node; don't change this\n",
    "    'hidden_channels': 32,   # How many intermediate dimensions to use in model\n",
    "                            # Can change this ^^\n",
    "    'num_additional': 4      # How many additional resin properties to include in the prediction\n",
    "                            # Corresponds to the number in get_IV_add\n",
    "}\n",
    "\n",
    "model = PolymerGNN_IV(**model_kwargs)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    optimizer = optimizer,\n",
    "    criterion = criterion,\n",
    "    dataset = dataset,\n",
    "    batch_size = 64,\n",
    "    epochs = 800\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenqueen/Desktop/Eastman_project-Confidential/Eastman_Project/PolymerGNN/polymerlearn/explain/explain_gnn.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(add_test).float())\n"
     ]
    }
   ],
   "source": [
    "from polymerlearn.explain import PolymerGNN_EXPLAIN, PolymerGNNExplainer\n",
    "\n",
    "mexplain = PolymerGNN_EXPLAIN(**model_kwargs)\n",
    "mexplain.load_state_dict(model.state_dict()) # Load weights from trained model over to explaining one\n",
    "\n",
    "explainer = PolymerGNNExplainer(mexplain)\n",
    "\n",
    "test_batch, Ytest, add_test = dataset.get_test()\n",
    "test_inds = dataset.test_maks\n",
    "\n",
    "exp_summary = []\n",
    "\n",
    "for i in range(Ytest.shape[0]):\n",
    "    scores = explainer.get_explanation(test_batch[i], add_test[i])\n",
    "    scores['A'] = torch.sum(scores['A'], dim = 1)\n",
    "    scores['G'] = torch.sum(scores['G'], dim = 1)\n",
    "    scores['table_ind'] = test_inds[i]\n",
    "\n",
    "    exp_summary.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: 18022.0 \t log(Table): 9.799348512794982 \t Add test: 9.799348831176758\n",
      "Table: 41426.0 \t log(Table): 10.631663982015468 \t Add test: 10.631664276123047\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 45044.0 \t log(Table): 10.715395068816916 \t Add test: 10.715394973754883\n",
      "Table: 97437.0 \t log(Table): 11.486961294292419 \t Add test: 11.486961364746094\n",
      "Table: 47245.0 \t log(Table): 10.763102107216726 \t Add test: 10.763102531433105\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 17656.0 \t log(Table): 9.778830947936573 \t Add test: 9.778830528259277\n",
      "Table: 33774.0 \t log(Table): 10.427446554692082 \t Add test: 10.427446365356445\n",
      "Table: 60549.0 \t log(Table): 11.01120823356823 \t Add test: 11.011208534240723\n",
      "Table: 21043.0 \t log(Table): 9.954323242238624 \t Add test: 9.954322814941406\n",
      "Table: 65814.0 \t log(Table): 11.09458786063939 \t Add test: 11.094588279724121\n",
      "Table: 29554.0 \t log(Table): 10.293974377463579 \t Add test: 10.293973922729492\n",
      "Table: 20882.0 \t log(Table): 9.946642822850055 \t Add test: 9.946642875671387\n",
      "Table: 62276.0 \t log(Table): 11.039331397809566 \t Add test: 11.039331436157227\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 47381.0 \t log(Table): 10.765976583441928 \t Add test: 10.765976905822754\n",
      "Table: 116309.0 \t log(Table): 11.664005721583116 \t Add test: 11.664005279541016\n",
      "Table: 41046.0 \t log(Table): 10.622448667989053 \t Add test: 10.622448921203613\n",
      "Table: 35064.0 \t log(Table): 10.464930242098646 \t Add test: 10.464930534362793\n",
      "Table: 18028.0 \t log(Table): 9.799681383810539 \t Add test: 9.799681663513184\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 19735.0 \t log(Table): 9.890148988096948 \t Add test: 9.890149116516113\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 29199.0 \t log(Table): 10.281889741094604 \t Add test: 10.281889915466309\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 56039.0 \t log(Table): 10.933803155894871 \t Add test: 10.93380355834961\n",
      "Table: 14525.0 \t log(Table): 9.583626581720113 \t Add test: 9.583626747131348\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 38228.0 \t log(Table): 10.55132351038607 \t Add test: 10.551323890686035\n",
      "Table: 19288.0 \t log(Table): 9.867238419861007 \t Add test: 9.86723804473877\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 28004.0 \t log(Table): 10.240102636097088 \t Add test: 10.240102767944336\n",
      "Table: 88424.0 \t log(Table): 11.389898704990792 \t Add test: 11.389898300170898\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 18962.0 \t log(Table): 9.850192255477904 \t Add test: 9.850192070007324\n",
      "Table: 7775.0 \t log(Table): 8.958668737047434 \t Add test: 8.95866870880127\n",
      "Table: 9195.0 \t log(Table): 9.126415137038421 \t Add test: 9.126415252685547\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 20124.0 \t log(Table): 9.909668411611209 \t Add test: 9.90966796875\n",
      "Table: 86128.0 \t log(Table): 11.363589840800739 \t Add test: 11.363590240478516\n",
      "Table: 93058.0 \t log(Table): 11.44097823365684 \t Add test: 11.440978050231934\n",
      "Table: 55003.0 \t log(Table): 10.915143008181603 \t Add test: 10.915143013000488\n",
      "Table: 20617.0 \t log(Table): 9.933871257170567 \t Add test: 9.933871269226074\n",
      "Table: nan \t log(Table): nan \t Add test: 10.427446365356445\n",
      "Table: 23771.0 \t log(Table): 10.076221629305257 \t Add test: 10.076221466064453\n",
      "Table: 30164.0 \t log(Table): 10.314404439322537 \t Add test: 10.314404487609863\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_mask = data.loc[data['IV'].notna(),:]\n",
    "\n",
    "for i in range(len(test_inds)):\n",
    "    j = test_inds[i]\n",
    "    mw_table = data_mask['Mw (PS)'].iloc[j]\n",
    "    # Test mw from add_test\n",
    "    mw_add_test = add_test[i,0]\n",
    "\n",
    "    print('Table: {} \\t log(Table): {} \\t Add test: {}'.format(\n",
    "        mw_table, np.log(mw_table), mw_add_test.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.8878821134567261, -1.0357295274734497, -0.4294568598270416, -2.0798838138580322, -1.716792345046997, -1.7568775415420532, -0.47344210743904114, -0.6670295000076294, -0.8664342761039734, -1.5350826978683472, -0.7446939945220947, -2.2649717330932617, -0.899152398109436, -0.632922887802124, -1.637803554534912, -0.6790949106216431, -1.4522514343261719, -1.697495937347412, -2.3722074031829834, -1.516371488571167, -0.6643494367599487, -0.5550310611724854, -0.7443832755088806, -0.6352322697639465, -0.4272572100162506, -1.1757748126983643, -0.371645987033844, -2.1001482009887695, -0.5719149708747864, -0.5653843283653259, -1.7718409299850464, -0.5770406723022461, -0.5178261399269104, -0.9155601263046265, -2.273362398147583, -0.9710496664047241, -0.49641841650009155, -1.1031956672668457, -0.7165290117263794, -1.0446813106536865, -0.653941810131073, -1.13113534450531, -2.244901180267334, -1.4169310331344604, -2.206786870956421, -0.695833146572113, -0.4037015736103058, -0.8395153284072876, -1.0066689252853394]\n",
      "[-0.8878821134567261, -1.0357295274734497, -0.42945683002471924, -2.0798838138580322, -1.716792345046997, -1.7568775415420532, -0.47344210743904114, -0.6670294404029846, -0.8664342761039734, -1.5350826978683472, -0.7446940541267395, -2.2649717330932617, -0.8991522789001465, -0.6329228281974792, -1.637803554534912, -0.6790949106216431, -1.4522514343261719, -1.6974958181381226, -2.3722074031829834, -1.516371488571167, -0.6643494367599487, -0.5550310015678406, -0.7443832159042358, -0.6352323293685913, -0.4272571802139282, -1.1757748126983643, -0.371645987033844, -2.1001482009887695, -0.5719149112701416, -0.5653843283653259, -1.771841049194336, -0.5770406723022461, -0.5178261995315552, -0.9155600666999817, -2.273362398147583, -0.9710496664047241, -0.49641841650009155, -1.1031956672668457, -0.7165290117263794, -1.0446813106536865, -0.6539418697357178, -1.13113534450531, -2.244901180267334, -1.4169310331344604, -2.206786870956421, -0.6958332061767578, -0.4037016034126282, -0.8395154476165771, -1.0066689252853394]\n",
      "[1.0391219854354858, 1.4881130456924438, 0.5755305290222168, 2.825047731399536, 2.4909889698028564, 2.3071584701538086, 0.6315327882766724, 0.8299001455307007, 1.1550045013427734, 2.1872003078460693, 0.9770840406417847, 3.0863194465637207, 1.157400369644165, 0.7761979699134827, 2.371814727783203, 0.94269198179245, 2.085409164428711, 2.4690239429473877, 3.240626811981201, 1.6726226806640625, 0.8322977423667908, 0.7697827219963074, 1.0333080291748047, 0.7821510434150696, 0.5699259638786316, 1.4950401782989502, 0.48271259665489197, 2.7460293769836426, 0.6818888783454895, 0.7841419577598572, 2.4777591228485107, 0.7283717393875122, 0.6907375454902649, 1.2160015106201172, 3.055600166320801, 1.2944788932800293, 0.6255237460136414, 1.0417226552963257, 0.6947116255760193, 1.4488877058029175, 0.9069640636444092, 1.4890202283859253, 3.0074987411499023, 2.0505383014678955, 2.939455032348633, 0.8794628977775574, 0.5385123491287231, 1.1118735074996948, 1.1233537197113037]\n",
      "[-0.029572611674666405, -0.09821107238531113, -0.082550048828125, -0.09591835737228394, -0.0813969150185585, -0.042666252702474594, -0.08463376760482788, -0.017386306077241898, -0.03119850903749466, -0.11392807960510254, -0.018581077456474304, -0.10120777040719986, -0.020352283492684364, -0.031572628766298294, -0.14329664409160614, -0.06136297434568405, -0.09844528138637543, -0.1177050843834877, -0.12434546649456024, 0.00234248791821301, -0.007588636130094528, -0.07369792461395264, -0.06846830993890762, -0.03660163655877113, -0.08471556007862091, -0.04055086150765419, -0.06792627274990082, -0.043759919703006744, -0.06174532324075699, -0.06637568771839142, -0.1735873967409134, -0.0656030923128128, -0.0955481082201004, -0.045299939811229706, -0.05319906398653984, -0.0650796890258789, -0.05243241786956787, 0.0052937776781618595, -0.010772754438221455, -0.11073096841573715, -0.08273909986019135, -0.004458892159163952, -0.05248279124498367, -0.0465846061706543, -0.053401101380586624, -0.027893081307411194, -0.06844343990087509, -0.026574095711112022, -0.02085299603641033]\n",
      "[-0.23681262135505676, -0.24145391583442688, -0.16477800905704498, -0.6181857585906982, -0.5338180661201477, -0.44729164242744446, -0.18416275084018707, -0.27855268120765686, -0.317195862531662, -0.5397773385047913, -0.260864794254303, -0.6522755026817322, -0.2896974980831146, -0.23640787601470947, -0.6211885213851929, -0.23784959316253662, -0.5225630402565002, -0.5062261819839478, -0.7440840601921082, -0.28692248463630676, -0.25061655044555664, -0.1979760229587555, -0.23440523445606232, -0.21538008749485016, -0.16430474817752838, -0.32472464442253113, -0.1593601256608963, -0.5177026987075806, -0.2136038839817047, -0.20643115043640137, -0.1472054421901703, -0.19907474517822266, -0.18083328008651733, -0.25562891364097595, -0.5596108436584473, -0.2820916175842285, -0.17196768522262573, -0.14216621220111847, -0.17043046653270721, -0.1817871630191803, -0.2157253921031952, -0.38283392786979675, -0.5520763397216797, -0.45512863993644714, -0.5618022084236145, -0.2856031358242035, -0.1755497306585312, -0.23017999529838562, -0.28057509660720825]\n",
      "[-0.0, 0.0, 0.0, -0.0, 0.01659758761525154, -0.0, 0.0, 0.010297844186425209, -0.036525607109069824, -0.0, -0.00037434507976286113, -0.0, 0.017696339637041092, -0.032970186322927475, 0.0, 0.0, 0.0, 0.05412055179476738, -0.0, -0.0, -0.027131255716085434, 0.0, 0.0, 0.006125602871179581, 0.0, -0.0, 0.0, -0.0, 0.019707446917891502, 0.0, 0.0, 0.015255110338330269, 0.0, -0.0004528774006757885, -0.0, 0.019689608365297318, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, -0.0, 0.011970706284046173, -0.0, 0.011568887159228325, 0.0, -0.0170106403529644, -0.0]\n",
      "{'SA': [-0.42976903915405273, -0.5790528059005737, -0.5000565648078918, -1.076910138130188, -1.516371488571167, -0.44399604201316833, -1.1031956672668457, -0.7293716669082642, -0.44832727313041687], 'ADP': [-0.14439640939235687, -2.0798838138580322, -0.31279081106185913, -1.7568775415420532, -2.3722074031829834, -1.1757748126983643, -2.1001482009887695, -0.1489695906639099, -0.2597346603870392, -2.206786870956421], 'DDDA': [], 'AZL': [], '1,4-CHDA': [-2.2649717330932617, -0.3077094852924347, -0.292565256357193], '1,4-CHDA (95% trans)': [], '1,3-CHDA': [-0.8878821134567261, -2.273362398147583], '1,2-CHDA': [-2.244901180267334], 'HHPA': [], 'PA': [-0.7165290117263794, -1.0066689252853394], 'IPA': [-0.8913331031799316, -0.42806506156921387, -1.4040014743804932, -0.47189027070999146, -0.23707015812397003, -0.28724056482315063, -1.5350826978683472, -0.24435760080814362, -0.8974311351776123, -0.632922887802124, -0.6764309406280518, -0.6205858588218689, -0.2202453464269638, -0.5532168745994568, -0.7414631843566895, -0.6340336203575134, -0.42585673928260803, -0.37037160992622375, -0.5719149708747864, -0.5635362863540649, -1.764413595199585, -0.5751492381095886, -0.5161288380622864, -0.6078506112098694, -0.8220800757408142, -0.49479126930236816, -1.041266679763794, -0.6518043279647827, -0.4011605381965637, -1.1571964025497437, -0.24730733036994934, -0.4023783504962921, -0.546950101852417], 'TPA': [-0.0013917911564931273, -0.001551840454339981, -0.00019034373690374196, -0.0001409249089192599, -0.0002798534987960011, -0.0017212677048519254, -1.637803554534912, -0.002663965104147792, -1.4522514343261719, -0.00010805625061038882, -0.0018141911132261157, -0.0029201137367635965, -0.0011986341560259461, -0.0014004562981426716, -0.0012743774568662047, -0.0018480324652045965, -0.007427390664815903, -0.001891413819976151, -0.0016973220044746995, 0.0, -0.00162715173792094, -0.0034146762918680906, -0.0021374933421611786, -0.0006030657095834613, -0.0001985629933187738, -0.0013232204364612699, 0.0], 'TMA': []}\n",
      "{'EG': [-1.7568775415420532, -1.4522514343261719], 'DEG': [-1.637803554534912, -2.3722074031829834], '1,3-PROP': [-0.7165290117263794], '1,4-BUT': [-2.1001482009887695, -2.273362398147583, -2.244901180267334], 'HDO': [-1.5350826978683472, -0.008853178471326828, -2.206786870956421], 'NPG': [-2.0798838138580322, -2.2649717330932617, -0.20120716094970703, -0.18695801496505737, -0.41779622435569763, -0.29368966817855835], '1,4-CHDM': [-0.24326306581497192, -0.07998119294643402, -0.27069613337516785, -0.3534524440765381, -0.030575605109333992, -0.4082919955253601, -0.07315272092819214, -1.516371488571167, -0.4475325345993042, -0.29715532064437866, -0.02335585094988346, 0.03145448490977287, -0.27623727917671204, -0.2960732877254486, -0.42212390899658203, -1.1031956672668457, -0.6276122331619263, -0.3928680419921875, -0.71333909034729], '1,3-CHDM': [-0.5308041572570801, -0.3334454596042633, -0.244289368391037, -0.21579670906066895, -0.3396654427051544, -0.2838331460952759, -0.2308279573917389, -1.0066689252853394], 'TMCD': [-0.8878821134567261, -0.3902451992034912, -0.18619376420974731, -0.2027459740638733, -0.312237948179245, -0.357622355222702, -0.27080291509628296, -0.22158555686473846, -0.29685068130493164, -0.059463806450366974, -0.18296782672405243, -1.1757748126983643, -0.15584927797317505, -0.2257188856601715, -0.715753972530365, -0.04890770465135574, -0.22175291180610657, -0.4048748314380646, -0.3988257348537445, -0.21258528530597687, -0.41706904768943787, -0.2610738277435303, -0.6429628133773804, -0.1728736311197281, -0.1737084984779358], 'TMP': [-0.24051892757415771, -0.0923316478729248, -0.3356301188468933, -0.07900364696979523, -0.09140637516975403, -0.11688730120658875, -0.20516568422317505, -0.25019481778144836, -0.07740606367588043, -0.08805975317955017, -0.06493766605854034, -0.08856137841939926, -0.0682462602853775, -0.09102358669042587, -0.09631872922182083, -0.3721172511577606], 'MPD': [-0.6454843282699585, -1.3962922096252441, -0.4784579575061798, -0.4501235783100128, -0.48545992374420166, -1.4191774129867554, -0.4604993164539337, -0.5039776563644409, -0.682944655418396, -0.49911877512931824], 'TCDDM': [-0.09623986482620239, -0.41415464878082275, -1.078688383102417, -0.10039570927619934]}\n"
     ]
    }
   ],
   "source": [
    "# Summarize importance scores:\n",
    "\n",
    "# Mw summary:\n",
    "acid_scores = []\n",
    "glycol_scores = []\n",
    "\n",
    "mw_scores = []\n",
    "an_scores = []\n",
    "ohn_scores = []\n",
    "tmp_scores = []\n",
    "\n",
    "def get_AG_info(data, ac = (20,33), gc = (34,46)):\n",
    "\n",
    "    # Decompose the data into included names\n",
    "    acid_names = pd.Series([c[1:] for c in data.columns[ac[0]:ac[1]].tolist()])\n",
    "    glycol_names = pd.Series([c[1:] for c in data.columns[gc[0]:gc[1]].tolist()])\n",
    "\n",
    "    # Holds all names of acids and glycols\n",
    "    acid_included = []\n",
    "    glycol_included = []\n",
    "\n",
    "    # Keep track of percents in each acid, glycol\n",
    "    acid_pcts = []\n",
    "    glycol_pcts = []\n",
    "\n",
    "    # Get relevant names and percentages of acid/glycols\n",
    "    for i in range(data.shape[0]):\n",
    "\n",
    "        acid_hit = (data.iloc[i,ac[0]:ac[1]].to_numpy() > 0)\n",
    "        glycol_hit = (data.iloc[i,gc[0]:gc[1]].to_numpy() > 0)\n",
    "\n",
    "        # Add to percentage lists:\n",
    "        acid_pcts.append(data.iloc[i,ac[0]:ac[1]][acid_hit].tolist())\n",
    "        glycol_pcts.append(data.iloc[i,gc[0]:gc[1]][glycol_hit].tolist()) \n",
    "\n",
    "        acid_pos = acid_names[np.argwhere(acid_hit).flatten()].tolist()\n",
    "        glycol_pos = glycol_names[np.argwhere(glycol_hit).flatten()].tolist()\n",
    "\n",
    "        acid_included.append(acid_pos)\n",
    "        glycol_included.append(glycol_pos)\n",
    "\n",
    "    return acid_included, glycol_included, acid_pcts, glycol_pcts\n",
    "\n",
    "acid_names = pd.Series([c[1:] for c in data_mask.columns[20:33].tolist()])\n",
    "glycol_names = pd.Series([c[1:] for c in data_mask.columns[34:46].tolist()])\n",
    "acids, glycols, _, _ = get_AG_info(data_mask)\n",
    "\n",
    "acid_key = {a:[] for a in acid_names}\n",
    "glycol_key = {g:[] for g in glycol_names}\n",
    "\n",
    "for i in range(len(exp_summary)):\n",
    "\n",
    "    df_ind = exp_summary[i]['table_ind']\n",
    "\n",
    "    for a in range(len(acids[df_ind])):\n",
    "        acid_key[acids[df_ind][a]].append(exp_summary[i]['A'][a].item()) \n",
    "    \n",
    "    for g in range(len(glycols[df_ind])):\n",
    "        glycol_key[glycols[df_ind][g]].append(exp_summary[i]['G'][g].item()) \n",
    "\n",
    "    acid_scores.append(torch.sum(exp_summary[i]['A']).item())\n",
    "    glycol_scores.append(torch.sum(exp_summary[i]['G']).item())\n",
    "\n",
    "    mw_scores.append(exp_summary[i]['add'][0].item())\n",
    "    an_scores.append(exp_summary[i]['add'][1].item())\n",
    "    ohn_scores.append(exp_summary[i]['add'][2].item())\n",
    "    tmp_scores.append(exp_summary[i]['add'][3].item())\n",
    "\n",
    "print(acid_scores)\n",
    "print(glycol_scores)\n",
    "\n",
    "print(mw_scores)\n",
    "print(an_scores)\n",
    "print(ohn_scores)\n",
    "print(tmp_scores)\n",
    "\n",
    "print(acid_key)\n",
    "print(glycol_key)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
